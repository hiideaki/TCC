{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../'\n",
    "\n",
    "PROCESSED_DATA_PATH = os.path.join(PATH, 'data/processed/')\n",
    "\n",
    "DF_PRIME_TRUNCATED_PATH = os.path.join(PROCESSED_DATA_PATH, 'df_prime_truncated.pkl')\n",
    "DF_USP_TRUNCATED_PATH = os.path.join(PROCESSED_DATA_PATH, 'df_usp_truncated_clean.pkl')\n",
    "\n",
    "\n",
    "STOPWORDS = utils.get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fakebr = pkl.load(open(DF_USP_TRUNCATED_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../'\n",
    "\n",
    "PROCESSED_DATA_PATH = os.path.join(PATH, 'data/processed/')\n",
    "\n",
    "DF_PRIME_TRUNCATED_PATH = os.path.join(PROCESSED_DATA_PATH, 'df_prime_truncated.pkl')\n",
    "DF_USP_TRUNCATED_PATH = os.path.join(PROCESSED_DATA_PATH, 'df_usp_truncated_clean.pkl')\n",
    "\n",
    "\n",
    "STOPWORDS = utils.get_stopwords()\n",
    "\n",
    "df_prime = pkl.load(open(DF_PRIME_TRUNCATED_PATH, 'rb'))\n",
    "\n",
    "df_fakebr = pkl.load(open(DF_USP_TRUNCATED_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prime, X_test_prime, y_train_prime, y_test_prime = \\\n",
    "    train_test_split(df_prime['TEXT_CLEAN'], df_prime['FAKE'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fakebr, X_test_fakebr, y_train_fakebr, y_test_fakebr = \\\n",
    "    train_test_split(df_fakebr['TEXT_CLEAN'], df_fakebr['FAKE'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = list(X_train_fakebr) + list(X_train_prime)\n",
    "y_train_all = list(y_train_fakebr) + list(y_train_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aux = list(X_test_fakebr) + list(X_test_prime)\n",
    "y_aux = list(y_test_fakebr) + list(y_test_prime)\n",
    "src_aux = len(list(y_test_fakebr)) * ['fakebr'] + len(list(y_test_prime)) * ['prime']\n",
    "\n",
    "df_resample = pd.DataFrame({'X': X_aux, 'y': y_aux, 'src': src_aux})\n",
    "rus = RandomUnderSampler()\n",
    "resampled = rus.fit_resample(df_resample[['X', 'y']], df_resample['src'])\n",
    "\n",
    "X_test_all = [item[0] for item in resampled[0]]\n",
    "y_test_all = [item[1] for item in resampled[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_list = [\n",
    "    TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9, max_features=10000)\n",
    "]\n",
    "\n",
    "\n",
    "pipe = Pipeline([('vect', vectorizer_list[0]), ('clf', LogisticRegression())])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'clf__solver': ['lbfgs', 'saga'],\n",
    "    'clf__tol': [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   59.2s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done 280 out of 280 | elapsed:  3.2min finished\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=0.9,\n",
       "                                                        max_features=10000,\n",
       "                                                        min_df=5,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_word...\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
       "                         'clf__solver': ['lbfgs', 'saga'],\n",
       "                         'clf__tol': [1e-05, 0.0001, 0.001, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv = GridSearchCV(pipe, param_grid, cv=5, n_jobs=4, verbose=10)\n",
    "gscv.fit(X_train_fakebr, y_train_fakebr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.903):\n",
      "{'clf__C': 100, 'clf__solver': 'saga', 'clf__tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter (CV score={:.3f}):\".format(gscv.best_score_))\n",
    "print(gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kwargs = {'C': 100, 'solver': 'saga', 'tol': 0.0001}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.9, max_features=10000)), \n",
    "    ('clf', LogisticRegression(**clf_kwargs))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=10000,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train_fakebr, y_train_fakebr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       371\n",
      "           1       0.88      0.87      0.87       367\n",
      "\n",
      "    accuracy                           0.87       738\n",
      "   macro avg       0.87      0.87      0.87       738\n",
      "weighted avg       0.87      0.87      0.87       738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_all, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[326  45]\n",
      " [ 48 319]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_all, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8766937669376694"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_all, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitar que para fins de treinamento os conjuntos seriam utilizados separadamente durante todo o trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
